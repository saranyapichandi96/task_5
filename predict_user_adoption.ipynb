{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQBm2Ag3Me/BxXJMdGUqEj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saranyapichandi96/task_5/blob/main/predict_user_adoption.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJDIXOrXBOMJ"
      },
      "outputs": [],
      "source": [
        "# importing all the required libraries and modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "from datetime import datetime, timedelta\n",
        "import scipy.stats\n",
        "import matplotlib.dates as mdates\n",
        "import plotly.graph_objects as go\n",
        "plt.style.use('bmh')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data/takehome_users.csv') as f:\n",
        "    print(f)\n",
        "    #for text in f:\n",
        "        #print(text)\n",
        "with open('data/takehome_user_engagement.csv') as f:\n",
        "    print(f)"
      ],
      "metadata": {
        "id": "H4nTumFBBhte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = pd.read_csv('data/takehome_users.csv', parse_dates = ['creation_time'], \n",
        "                    encoding = \"cp1252\")\n",
        "user_eng = pd.read_csv('data/takehome_user_engagement.csv',  parse_dates = ['time_stamp'], \n",
        "                       encoding = \"cp1252\")"
      ],
      "metadata": {
        "id": "m0AsTY7_BqQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.info()"
      ],
      "metadata": {
        "id": "C9hO65yVBtk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.describe().T\n"
      ],
      "metadata": {
        "id": "rm1z9SdTBwl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_eng.info()"
      ],
      "metadata": {
        "id": "Sda5jn1GB1qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_eng.describe().T"
      ],
      "metadata": {
        "id": "m1KLrcLzB6wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users.head()"
      ],
      "metadata": {
        "id": "Rev0eVXqB7sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_eng.head()"
      ],
      "metadata": {
        "id": "0QX7TcPlCAKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users['last_session_creation_time'] = pd.to_datetime(users['last_session_creation_time'] ,unit='s')\n",
        "users.last_session_creation_time.dtypes\n",
        "users.head(3)"
      ],
      "metadata": {
        "id": "JwBoHn7OCA1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users['last_session_creation_time'].min(), users['last_session_creation_time'].max()\n"
      ],
      "metadata": {
        "id": "q8ZT2uyMCO3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = user_eng.copy()\n",
        "df['date'] = pd.to_datetime(df.time_stamp.dt.date)\n",
        "def rolling_count(df_group, frequency):\n",
        "    return df_group.rolling(frequency, on='date')['user_id'].count()\n",
        "df['visits_7_days'] = df.groupby('user_id', as_index=False, group_keys=False).apply(rolling_count, '7D')\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "vqeVV9jaCaSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.visits_7_days >= 3.0]"
      ],
      "metadata": {
        "id": "kpjWgxMuClgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_adopted.adopted_user.value_counts()"
      ],
      "metadata": {
        "id": "MlGIJuAbCmRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_adopted.set_index(\"object_id\", inplace = True)"
      ],
      "metadata": {
        "id": "Gq5lVwj_CvFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users = users.join(user_adopted, on = 'object_id', how='left')\n",
        "df_users.head()\n",
        "object_id\tcreation_time\t"
      ],
      "metadata": {
        "id": "QVytAmCnCv_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users.info()"
      ],
      "metadata": {
        "id": "_BouiNNECzET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The null values in the adopted_user and last_session_creation_time can be filled in with 0 because we can assume that those users aren't adopted users.\n",
        "\n",
        "df_users['last_session_creation_time'].fillna(0, inplace = True)\n",
        "df_users['adopted_user'].fillna(0, inplace = True)\n",
        "df_users.describe().T"
      ],
      "metadata": {
        "id": "PpK6ZxHzC6SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see if we can extract useful information from the email variable like it's domain.\n",
        "\n",
        "df_users['email_domain'] = df_users.email.apply(lambda x: x.split('@')[1])\n",
        "df_users['email_domain'].value_counts()"
      ],
      "metadata": {
        "id": "gtgn69s0C8AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Also checking the creation_source for NON NULL invited_by_user column\n",
        "df_users[~df_users.invited_by_user_id.isnull()].creation_source.unique()\n",
        "array(['GUEST_INVITE', 'ORG_INVITE'], dtype=object)"
      ],
      "metadata": {
        "id": "GbG3UuP6DAF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "There are too many email domains and most of them seem fake domains so it's good to drop the column entirely. We can also drop the name and object_id columns.\n",
        "\n",
        "And for the invited_by_user_id let's convert the NULL values to 0 because the column has a Non Null value only if the creation_source was a GUEST_INVITE or a ORG_INVITE anyways.\n",
        "\n",
        "For the creation_time column let's add a column which calculates how old the account is, i.e. the number of days since the account was created.\n",
        "\n",
        "And since last_session_creation_time can be removed as well because it was in a sense used to create the adopted_user column.\n",
        "'''\n",
        "\n",
        "df_users.drop(['object_id', 'name', 'email', 'email_domain'], axis = 1, inplace = True)\n",
        "df_users.invited_by_user_id.fillna(0, inplace=True)\n",
        "df_users['days_since_creation'] = (user_eng.time_stamp.max() - df_users.creation_time).dt.days\n",
        "df_users.drop(['creation_time', 'last_session_creation_time'], axis = 1, inplace = True)\n"
      ],
      "metadata": {
        "id": "TOWfnNdADEYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's OneHotEncode the creation_source column."
      ],
      "metadata": {
        "id": "wOdGcMJ1DTaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users = pd.get_dummies(df_users, columns=['creation_source'])\n",
        "df_users.describe().T"
      ],
      "metadata": {
        "id": "iz2S5yccDVZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in ['opted_in_to_mailing_list', 'enabled_for_marketing_drip', 'creation_source_GUEST_INVITE',\n",
        "           'creation_source_ORG_INVITE', 'creation_source_PERSONAL_PROJECTS', 'creation_source_SIGNUP',\n",
        "           'creation_source_SIGNUP_GOOGLE_AUTH']:\n",
        "    g = sns.FacetGrid(df_users, hue = \"adopted_user\", height=3, aspect=1.5,)\n",
        "    g.map(plt.hist, col, alpha=.5, bins = 20)\n",
        "    g.add_legend() "
      ],
      "metadata": {
        "id": "-8hPwnptDX0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df_users['days_since_creation'], kde = False, bins = 20, hue = 'adopted_user')"
      ],
      "metadata": {
        "id": "4p4LE3ueDckk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's try to fit Random Forest Regression model and find the feature importance. Since we will be using random forest using trees we don't need to scale any features.\n",
        "# Importing necessary packages\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score,\\\n",
        "precision_score, recall_score, f1_score\n",
        "def cv_optimize(model, parameters, Xtrain, ytrain, n_folds = 5):\n",
        "    \"\"\"\n",
        "    Cross validation. Function to hypertune the model \"model\" with the input paramete distribution using\n",
        "    \"parameters\" on the training data.\n",
        "    The output will be the best estimator whose average score on all folds will be best. \n",
        "    \"\"\"\n",
        "    clf = GridSearchCV(model, param_grid = parameters, cv = n_folds, scoring = 'accuracy')\n",
        "    t0 = time.time()\n",
        "    clf.fit(Xtrain, ytrain)\n",
        "    time_fit = time.time() - t0 \n",
        "    print('\\n\\n\\n=============================',type(model).__name__,'=================================\\n')\n",
        "    print(\"It takes %.3f seconds for tuning \" % (time_fit))\n",
        "     print(\"BEST PARAMS\", clf.best_params_)\n",
        "    best = clf.best_estimator_\n",
        "    return best\n",
        "    \n",
        "def do_classify(model, parameters, df, targetname, scale = True, cols_to_transform = 'numeric', \n",
        "                featurenames = 'all', train_size = 0.8):\n",
        "      \n",
        "    # Creating the X and y variables for our model\n",
        "    if featurenames == 'all':\n",
        "        X = df.drop([targetname], axis = 1)\n",
        "    else:\n",
        "        X = df[featurenames]\n",
        "        \n",
        "    y = df[targetname]\n",
        "    \n",
        "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size = train_size)\n",
        "\n",
        "    model = cv_optimize(model, parameters, Xtrain, ytrain)\n",
        "    t0 = time.time()\n",
        "   model = model.fit(Xtrain, ytrain)\n",
        "    time_fit = time.time() - t0 \n",
        "    print(\"It takes %.3f seconds for fitting\" % (time_fit))\n",
        "    training_accuracy = model.score(Xtrain, ytrain)\n",
        "    test_accuracy = model.score(Xtest, ytest)\n",
        "    precision = precision_score(ytest, model.predict(Xtest))\n",
        "    recall = recall_score(ytest, model.predict(Xtest))\n",
        "    AUC = roc_auc_score(ytest, model.predict_proba(Xtest)[:,1])\n",
        "            \n",
        "    print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
        "    print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
        "    print(\"Precision on test data:    {:0.2f}\".format(precision))\n",
        "    print(\"Recall on test data:       {:0.2f}\".format(recall))\n",
        "    print(\"AUC on test data:          {:0.2f}\".format(AUC))\n",
        "    print(\"=======Confusion Matrix=========\")\n",
        "    print(confusion_matrix(ytest, model.predict(Xtest)))\n",
        "    print(\"=======Classification report=======\")\n",
        "    print(classification_report(ytest, model.predict(Xtest)))\n",
        "    print(\"=\"*100)\n",
        "    print(\"=\"*100)\n",
        "    print(\"=\"*100)\n",
        "    return model, Xtrain, ytrain, Xtest, ytest\n",
        "# Random Forest model\n",
        "model_rf = RandomForestClassifier(class_weight='balanced') # adding balanced to handle the unbalanced data\n",
        "parameters_rf = {\n",
        "                 'n_estimators': [10, 25, 50, 75, 100],\n",
        "                 'criterion': [\"gini\", \"entropy\"],\n",
        "                 'max_depth': [3, 6, 10, 12],\n",
        "                 'max_features': ['auto', 'sqrt']\n",
        "                }\n",
        "model_rf, Xtrain, ytrain, Xtest, ytest = do_classify(model_rf, parameters_rf, \n",
        "                                                                  df_users, targetname = 'adopted_user')\n"
      ],
      "metadata": {
        "id": "_PzBBhVPDiRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_imp = pd.DataFrame({'importance':model_rf.feature_importances_})    \n",
        "feat_imp['feature'] = Xtrain.columns\n",
        "feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n",
        "    \n",
        "feat_imp.sort_values(by='importance', inplace=True)\n",
        "feat_imp = feat_imp.set_index('feature', drop=True)\n",
        "_ = feat_imp.plot.barh(title = 'Random Forest feature importance', figsize = (12,7))"
      ],
      "metadata": {
        "id": "Nb5FQ0GKEEv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The top 5 important features seem to be:\n",
        "\n",
        "days_since_creation\n",
        "org_id\n",
        "invited_by_user_id\n",
        "creation_source_PERSONAL_PROJECTS\n",
        "opted_in_to_mailing_list\n",
        "Furture possible work: We can also add a feature which calculates the difference between the creation date of the account and the first login of the user.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "iT-ZSNv2EJ9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}